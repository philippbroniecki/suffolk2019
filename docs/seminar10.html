<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Prediction and assessing prediction accuracy | Suffolk 2019</title>
  <meta name="description" content="Chapter 10 Prediction and assessing prediction accuracy | Suffolk 2019" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Prediction and assessing prediction accuracy | Suffolk 2019" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Prediction and assessing prediction accuracy | Suffolk 2019" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="seminar9.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="lib/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="js/codefolding.js"></script>


<script>
$(document).ready(function () {
  window.initializeCodeFolding();
});
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="lib\bootstrap\3.3.7\css\bootstrap.min.css" type="text/css" />
<link rel="stylesheet" href="css\readthedocs.css" type="text/css" />
<link rel="stylesheet" href="css\custom.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this course</a></li>
<li class="chapter" data-level="1" data-path="seminar1.html"><a href="seminar1.html"><i class="fa fa-check"></i><b>1</b> Introduction to R and RStudio</a><ul>
<li class="chapter" data-level="1.1" data-path="seminar1.html"><a href="seminar1.html#learning-objectives"><i class="fa fa-check"></i><b>1.1</b> Learning objectives</a><ul>
<li class="chapter" data-level="1.1.1" data-path="seminar1.html"><a href="seminar1.html#what-is-r"><i class="fa fa-check"></i><b>1.1.1</b> What is R?</a></li>
<li class="chapter" data-level="1.1.2" data-path="seminar1.html"><a href="seminar1.html#rstudio"><i class="fa fa-check"></i><b>1.1.2</b> RStudio</a></li>
<li class="chapter" data-level="1.1.3" data-path="seminar1.html"><a href="seminar1.html#console"><i class="fa fa-check"></i><b>1.1.3</b> Console</a></li>
<li class="chapter" data-level="1.1.4" data-path="seminar1.html"><a href="seminar1.html#scripts"><i class="fa fa-check"></i><b>1.1.4</b> Scripts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="seminar2.html"><a href="seminar2.html"><i class="fa fa-check"></i><b>2</b> R-syntax, data structures and types</a><ul>
<li class="chapter" data-level="2.1" data-path="seminar2.html"><a href="seminar2.html#seminar"><i class="fa fa-check"></i><b>2.1</b> Seminar</a><ul>
<li class="chapter" data-level="2.1.1" data-path="seminar2.html"><a href="seminar2.html#functions"><i class="fa fa-check"></i><b>2.1.1</b> Functions</a></li>
<li class="chapter" data-level="2.1.2" data-path="seminar2.html"><a href="seminar2.html#getting-help"><i class="fa fa-check"></i><b>2.1.2</b> Getting Help</a></li>
<li class="chapter" data-level="2.1.3" data-path="seminar2.html"><a href="seminar2.html#the-assignment-operator"><i class="fa fa-check"></i><b>2.1.3</b> The Assignment Operator</a></li>
<li class="chapter" data-level="2.1.4" data-path="seminar2.html"><a href="seminar2.html#vectors-and-subsetting"><i class="fa fa-check"></i><b>2.1.4</b> Vectors and subsetting</a></li>
<li class="chapter" data-level="2.1.5" data-path="seminar2.html"><a href="seminar2.html#matrices"><i class="fa fa-check"></i><b>2.1.5</b> Matrices</a></li>
<li class="chapter" data-level="2.1.6" data-path="seminar2.html"><a href="seminar2.html#arrays"><i class="fa fa-check"></i><b>2.1.6</b> Arrays</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="seminar3.html"><a href="seminar3.html"><i class="fa fa-check"></i><b>3</b> Data import (from csv, txt, and excel)</a><ul>
<li class="chapter" data-level="3.1" data-path="seminar3.html"><a href="seminar3.html#seminar-1"><i class="fa fa-check"></i><b>3.1</b> Seminar</a><ul>
<li class="chapter" data-level="3.1.1" data-path="seminar3.html"><a href="seminar3.html#setting-up"><i class="fa fa-check"></i><b>3.1.1</b> Setting up</a></li>
<li class="chapter" data-level="3.1.2" data-path="seminar3.html"><a href="seminar3.html#loading-data"><i class="fa fa-check"></i><b>3.1.2</b> Loading data</a></li>
<li class="chapter" data-level="3.1.3" data-path="seminar3.html"><a href="seminar3.html#importing-a-dataset-in-.csv-format"><i class="fa fa-check"></i><b>3.1.3</b> Importing a dataset in <code>.csv</code> format</a></li>
<li class="chapter" data-level="3.1.4" data-path="seminar3.html"><a href="seminar3.html#importing-a-dataset-in-excel-xlsx-format"><i class="fa fa-check"></i><b>3.1.4</b> Importing a dataset in Excel (xlsx) format</a></li>
<li class="chapter" data-level="3.1.5" data-path="seminar3.html"><a href="seminar3.html#importing-a-dataset-in-rdata-format"><i class="fa fa-check"></i><b>3.1.5</b> Importing a dataset in RData format</a></li>
<li class="chapter" data-level="3.1.6" data-path="seminar3.html"><a href="seminar3.html#importing-a-dataset-in-.txt-format."><i class="fa fa-check"></i><b>3.1.6</b> Importing a dataset in .txt format.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="seminar4.html"><a href="seminar4.html"><i class="fa fa-check"></i><b>4</b> Creating, amending, exporting data frames</a><ul>
<li class="chapter" data-level="4.1" data-path="seminar4.html"><a href="seminar4.html#seminar-2"><i class="fa fa-check"></i><b>4.1</b> Seminar</a><ul>
<li class="chapter" data-level="4.1.1" data-path="seminar4.html"><a href="seminar4.html#creating-data-frames"><i class="fa fa-check"></i><b>4.1.1</b> Creating data frames</a></li>
<li class="chapter" data-level="4.1.2" data-path="seminar4.html"><a href="seminar4.html#working-with-data-frames"><i class="fa fa-check"></i><b>4.1.2</b> Working with data frames</a></li>
<li class="chapter" data-level="4.1.3" data-path="seminar4.html"><a href="seminar4.html#amending-data-frames"><i class="fa fa-check"></i><b>4.1.3</b> Amending data frames</a></li>
<li class="chapter" data-level="4.1.4" data-path="seminar4.html"><a href="seminar4.html#saving-data-frames"><i class="fa fa-check"></i><b>4.1.4</b> Saving data frames</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="seminar5.html"><a href="seminar5.html"><i class="fa fa-check"></i><b>5</b> Type coercion</a><ul>
<li class="chapter" data-level="5.1" data-path="seminar5.html"><a href="seminar5.html#seminar-3"><i class="fa fa-check"></i><b>5.1</b> Seminar</a><ul>
<li class="chapter" data-level="5.1.1" data-path="seminar5.html"><a href="seminar5.html#coerce-a-factor-to-character"><i class="fa fa-check"></i><b>5.1.1</b> Coerce a factor to character</a></li>
<li class="chapter" data-level="5.1.2" data-path="seminar5.html"><a href="seminar5.html#coerce-a-character-variable-into-a-factor-variable"><i class="fa fa-check"></i><b>5.1.2</b> Coerce a character variable into a factor variable</a></li>
<li class="chapter" data-level="5.1.3" data-path="seminar5.html"><a href="seminar5.html#coerce-a-character-variable-into-a-numeric-variable"><i class="fa fa-check"></i><b>5.1.3</b> Coerce a character variable into a numeric variable</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="seminar6.html"><a href="seminar6.html"><i class="fa fa-check"></i><b>6</b> Loops and conditions</a><ul>
<li class="chapter" data-level="6.1" data-path="seminar6.html"><a href="seminar6.html#seminar-4"><i class="fa fa-check"></i><b>6.1</b> Seminar</a><ul>
<li class="chapter" data-level="6.1.1" data-path="seminar6.html"><a href="seminar6.html#for-loops"><i class="fa fa-check"></i><b>6.1.1</b> For loops</a></li>
<li class="chapter" data-level="6.1.2" data-path="seminar6.html"><a href="seminar6.html#conditions"><i class="fa fa-check"></i><b>6.1.2</b> Conditions</a></li>
<li class="chapter" data-level="6.1.3" data-path="seminar6.html"><a href="seminar6.html#the-ifelse-function"><i class="fa fa-check"></i><b>6.1.3</b> The ifelse() function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="seminar7.html"><a href="seminar7.html"><i class="fa fa-check"></i><b>7</b> Visualising data</a><ul>
<li class="chapter" data-level="7.1" data-path="seminar7.html"><a href="seminar7.html#seminar-5"><i class="fa fa-check"></i><b>7.1</b> Seminar</a><ul>
<li class="chapter" data-level="7.1.1" data-path="seminar7.html"><a href="seminar7.html#plots"><i class="fa fa-check"></i><b>7.1.1</b> Plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="seminar8.html"><a href="seminar8.html"><i class="fa fa-check"></i><b>8</b> Correlations and differences in means</a><ul>
<li class="chapter" data-level="8.1" data-path="seminar8.html"><a href="seminar8.html#seminar-6"><i class="fa fa-check"></i><b>8.1</b> Seminar</a><ul>
<li class="chapter" data-level="8.1.1" data-path="seminar8.html"><a href="seminar8.html#sample-variance-and-sample-standard-deviation"><i class="fa fa-check"></i><b>8.1.1</b> Sample Variance and Sample Standard Deviation</a></li>
<li class="chapter" data-level="8.1.2" data-path="seminar8.html"><a href="seminar8.html#t-test-for-the-sample-mean"><i class="fa fa-check"></i><b>8.1.2</b> T test for the sample mean</a></li>
<li class="chapter" data-level="8.1.3" data-path="seminar8.html"><a href="seminar8.html#loading-real-data"><i class="fa fa-check"></i><b>8.1.3</b> Loading real data</a></li>
<li class="chapter" data-level="8.1.4" data-path="seminar8.html"><a href="seminar8.html#t-test-one-sample-hypothesis-test-with-real-data"><i class="fa fa-check"></i><b>8.1.4</b> T-test (one-sample hypothesis test) with real data</a></li>
<li class="chapter" data-level="8.1.5" data-path="seminar8.html"><a href="seminar8.html#t-test-difference-in-means"><i class="fa fa-check"></i><b>8.1.5</b> T-test (difference in means)</a></li>
<li class="chapter" data-level="8.1.6" data-path="seminar8.html"><a href="seminar8.html#relationships-between-continuous-variables"><i class="fa fa-check"></i><b>8.1.6</b> Relationships between continuous variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="seminar9.html"><a href="seminar9.html"><i class="fa fa-check"></i><b>9</b> Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="seminar9.html"><a href="seminar9.html#seminar-7"><i class="fa fa-check"></i><b>9.1</b> Seminar</a><ul>
<li class="chapter" data-level="9.1.1" data-path="seminar9.html"><a href="seminar9.html#interpreting-regression-output"><i class="fa fa-check"></i><b>9.1.1</b> Interpreting Regression Output</a></li>
<li class="chapter" data-level="9.1.2" data-path="seminar9.html"><a href="seminar9.html#fitted-values"><i class="fa fa-check"></i><b>9.1.2</b> Fitted values</a></li>
<li class="chapter" data-level="9.1.3" data-path="seminar9.html"><a href="seminar9.html#additional-resources"><i class="fa fa-check"></i><b>9.1.3</b> Additional Resources</a></li>
<li class="chapter" data-level="9.1.4" data-path="seminar9.html"><a href="seminar9.html#multiple-linear-regression-models"><i class="fa fa-check"></i><b>9.1.4</b> Multiple linear regression models</a></li>
<li class="chapter" data-level="9.1.5" data-path="seminar9.html"><a href="seminar9.html#estimating-a-bivariate-regression"><i class="fa fa-check"></i><b>9.1.5</b> Estimating a Bivariate Regression</a></li>
<li class="chapter" data-level="9.1.6" data-path="seminar9.html"><a href="seminar9.html#non-linearities"><i class="fa fa-check"></i><b>9.1.6</b> Non-Linearities</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="seminar10.html"><a href="seminar10.html"><i class="fa fa-check"></i><b>10</b> Prediction and assessing prediction accuracy</a><ul>
<li class="chapter" data-level="10.1" data-path="seminar10.html"><a href="seminar10.html#seminar-8"><i class="fa fa-check"></i><b>10.1</b> Seminar</a><ul>
<li class="chapter" data-level="10.1.1" data-path="seminar10.html"><a href="seminar10.html#model-the-underlying-continuous-process"><i class="fa fa-check"></i><b>10.1.1</b> Model the Underlying Continuous Process</a></li>
<li class="chapter" data-level="10.1.2" data-path="seminar10.html"><a href="seminar10.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>10.1.2</b> Leave-One-Out-Cross-Validation</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Suffolk 2019</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="prediction-and-assessing-prediction-accuracy" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Prediction and assessing prediction accuracy</h1>
<div id="seminar-8" class="section level2">
<h2><span class="header-section-number">10.1</span> Seminar</h2>
<p>In this seminar we will cover classification, and assess model accuracy out-of-sample.</p>
<p>The non-western foreingers data is about the subjective perception of immigrants from non-western countries. The perception of immigrants from a context that is not similar to the one’s own ,is often used as a proxy for racism. Whether this is a fair measure or not is debatable but let’s examine the data from a survey carried out in Britain.</p>
<p>Let’s check the codebook of our data.</p>
<table>
<thead>
<tr class="header">
<th align="left">Variable</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">IMMBRIT</td>
<td align="left">Out of every 100 people in Britain, how many do you think are immigrants from non-western countries?</td>
</tr>
<tr class="even">
<td align="left">over.estimate</td>
<td align="left">1 if estimate is higher than 10.7%.</td>
</tr>
<tr class="odd">
<td align="left">RSex</td>
<td align="left">1 = male, 2 = female</td>
</tr>
<tr class="even">
<td align="left">RAge</td>
<td align="left">Age of respondent</td>
</tr>
<tr class="odd">
<td align="left">Househld</td>
<td align="left">Number of people living in respondent’s household</td>
</tr>
<tr class="even">
<td align="left">party identification</td>
<td align="left">1 = Conservatives, 2 = Labour, 3 = SNP, 4 = Greens, 5 = Ukip, 6 = BNP, 7 = other</td>
</tr>
<tr class="odd">
<td align="left">paper</td>
<td align="left">Do you normally read any daily morning newspaper 3+ times/week?</td>
</tr>
<tr class="even">
<td align="left">WWWhourspW</td>
<td align="left">How many hours WWW per week?</td>
</tr>
<tr class="odd">
<td align="left">religious</td>
<td align="left">Do you regard yourself as belonging to any particular religion?</td>
</tr>
<tr class="even">
<td align="left">employMonths</td>
<td align="left">How many mnths w. present employer?</td>
</tr>
<tr class="odd">
<td align="left">urban</td>
<td align="left">Population density, 4 categories (highest density is 4, lowest is 1)</td>
</tr>
<tr class="even">
<td align="left">health.good</td>
<td align="left">How is your health in general for someone of your age? (0: bad, 1: fair, 2: fairly good, 3: good)</td>
</tr>
<tr class="odd">
<td align="left">HHInc</td>
<td align="left">Income bands for household, high number = high HH income</td>
</tr>
</tbody>
</table>
<p>Let’s load the dataset.</p>
<pre class="sourceCode r"><code class="sourceCode r">df &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;non_western_immigrants.csv&quot;</span>, <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</code></pre>
<p>Declaring categorical variables.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># data manipulation</span>
df<span class="op">$</span>RSex &lt;-<span class="st"> </span><span class="kw">factor</span>(df<span class="op">$</span>RSex, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>))
df<span class="op">$</span>health.good &lt;-<span class="st"> </span><span class="kw">factor</span>(df<span class="op">$</span>health.good, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;bad&quot;</span>, <span class="st">&quot;fair&quot;</span>, <span class="st">&quot;fairly good&quot;</span>, <span class="st">&quot;good&quot;</span>) )</code></pre>
<p>We want to predict whether respondents over-estimate immigration from non-western contexts. We begin by normalizing our variables. Then we look at the distribution of the dependent variable. We check how well we could predict misperception of immigration in our sample without a statistical model.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create a copy of the original IMMBRIT variable (needed for classification with lm)</span>
df<span class="op">$</span>IMMBRIT_original_scale &lt;-<span class="st"> </span>df<span class="op">$</span>IMMBRIT

<span class="co"># our function for normalization</span>
our.norm &lt;-<span class="st"> </span><span class="cf">function</span>(x){
  <span class="kw">return</span>((x <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(x)) <span class="op">/</span><span class="st"> </span><span class="kw">sd</span>(x))
}

<span class="co"># continuous variables</span>
c.vars &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;IMMBRIT&quot;</span>, <span class="st">&quot;RAge&quot;</span>, <span class="st">&quot;Househld&quot;</span>, <span class="st">&quot;HHInc&quot;</span>, <span class="st">&quot;employMonths&quot;</span>, <span class="st">&quot;WWWhourspW&quot;</span>)

<span class="co"># normalize</span>
df[, c.vars] &lt;-<span class="st"> </span><span class="kw">apply</span>( df[, c.vars], <span class="dv">2</span>, our.norm )

<span class="co"># predict whether poeple overestimate rate of immigrants (i.e. more than 10.7%)</span>
<span class="kw">table</span>(df<span class="op">$</span>over.estimate)</code></pre>
<pre><code>
  0   1 
290 759 </code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># probability of misperception of immigration in the sample</span>
<span class="kw">mean</span>(df<span class="op">$</span>over.estimate)  </code></pre>
<pre><code>[1] 0.7235462</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># naive guess</span>
<span class="kw">median</span>(df<span class="op">$</span>over.estimate)</code></pre>
<pre><code>[1] 1</code></pre>
<p>Create dummy variables from our categorical variables.</p>
<pre class="sourceCode r"><code class="sourceCode r">df<span class="op">$</span>Cons &lt;-<span class="st"> </span><span class="kw">ifelse</span>(df<span class="op">$</span>party_self <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">yes =</span> <span class="dv">1</span>, <span class="dt">no =</span> <span class="dv">0</span>)
df<span class="op">$</span>Lab &lt;-<span class="st"> </span><span class="kw">ifelse</span>(df<span class="op">$</span>party_self <span class="op">==</span><span class="st"> </span><span class="dv">2</span>, <span class="dt">yes =</span> <span class="dv">1</span>, <span class="dt">no =</span> <span class="dv">0</span>)
df<span class="op">$</span>SNP &lt;-<span class="st"> </span><span class="kw">ifelse</span>(df<span class="op">$</span>party_self <span class="op">==</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">yes =</span> <span class="dv">1</span>, <span class="dt">no =</span> <span class="dv">0</span>)
df<span class="op">$</span>GP &lt;-<span class="st"> </span><span class="kw">ifelse</span>(df<span class="op">$</span>party_self <span class="op">==</span><span class="st"> </span><span class="dv">4</span>, <span class="dt">yes =</span> <span class="dv">1</span>, <span class="dt">no =</span> <span class="dv">0</span>)
df<span class="op">$</span>BNP &lt;-<span class="st"> </span><span class="kw">ifelse</span>(df<span class="op">$</span>party_self <span class="op">==</span><span class="st"> </span><span class="dv">6</span>, <span class="dt">yes =</span> <span class="dv">1</span>, <span class="dt">no =</span> <span class="dv">0</span>)
df<span class="op">$</span>Ukip &lt;-<span class="st"> </span><span class="kw">ifelse</span>(df<span class="op">$</span>party_self <span class="op">==</span><span class="st"> </span><span class="dv">5</span>, <span class="dt">yes =</span> <span class="dv">1</span>, <span class="dt">no =</span> <span class="dv">0</span>)
df<span class="op">$</span>party.other &lt;-<span class="st"> </span><span class="kw">ifelse</span>(df<span class="op">$</span>party_self <span class="op">==</span><span class="st"> </span><span class="dv">7</span>, <span class="dt">yes =</span> <span class="dv">1</span>, <span class="dt">no =</span> <span class="dv">0</span>)
df<span class="op">$</span>rural &lt;-<span class="st"> </span><span class="kw">ifelse</span>(df<span class="op">$</span>urban <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">yes =</span> <span class="dv">1</span>, <span class="dt">no =</span> <span class="dv">0</span>)
df<span class="op">$</span>partly.rural &lt;-<span class="st"> </span><span class="kw">ifelse</span>(df<span class="op">$</span>urban <span class="op">==</span><span class="st"> </span><span class="dv">2</span>, <span class="dt">yes =</span> <span class="dv">1</span>, <span class="dt">no =</span> <span class="dv">0</span>)
df<span class="op">$</span>urban &lt;-<span class="st"> </span><span class="kw">ifelse</span>(df<span class="op">$</span>urban <span class="op">==</span><span class="st"> </span><span class="dv">4</span>, <span class="dt">yes =</span> <span class="dv">1</span>, <span class="dt">no =</span> <span class="dv">0</span>)</code></pre>
<p>Now, we fit a logistic regression.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># run logistic regression</span>
m.log &lt;-<span class="st"> </span><span class="kw">glm</span>(over.estimate <span class="op">~</span><span class="st"> </span>RSex <span class="op">+</span><span class="st"> </span>RAge <span class="op">+</span><span class="st"> </span>Househld <span class="op">+</span><span class="st"> </span>Lab <span class="op">+</span><span class="st"> </span>SNP <span class="op">+</span><span class="st"> </span>Ukip <span class="op">+</span><span class="st"> </span>BNP <span class="op">+</span><span class="st"> </span>
<span class="st">               </span>GP <span class="op">+</span><span class="st"> </span>party.other <span class="op">+</span><span class="st"> </span>paper <span class="op">+</span><span class="st"> </span>WWWhourspW <span class="op">+</span><span class="st">  </span>religious <span class="op">+</span><span class="st"> </span>
<span class="st">               </span>employMonths <span class="op">+</span><span class="st"> </span>rural <span class="op">+</span><span class="st"> </span>partly.rural <span class="op">+</span><span class="st"> </span>urban <span class="op">+</span><span class="st"> </span>
<span class="st">               </span>health.good <span class="op">+</span><span class="st"> </span>HHInc, <span class="dt">data =</span> df,
             <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))
<span class="kw">summary</span>(m.log)</code></pre>
<pre><code>
Call:
glm(formula = over.estimate ~ RSex + RAge + Househld + Lab + 
    SNP + Ukip + BNP + GP + party.other + paper + WWWhourspW + 
    religious + employMonths + rural + partly.rural + urban + 
    health.good + HHInc, family = binomial(link = &quot;logit&quot;), data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2342  -1.1328   0.6142   0.8262   1.3815  

Coefficients:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)             0.72437    0.36094   2.007   0.0448 *  
RSexFemale              0.64030    0.15057   4.253 2.11e-05 ***
RAge                    0.01031    0.09073   0.114   0.9095    
Househld                0.02794    0.08121   0.344   0.7308    
Lab                    -0.31577    0.19964  -1.582   0.1137    
SNP                     1.85513    1.05603   1.757   0.0790 .  
Ukip                    0.05604    0.44846   0.125   0.9005    
BNP                     0.92131    0.57305   1.608   0.1079    
GP                     -0.51315    0.46574  -1.102   0.2706    
party.other             0.12542    0.18760   0.669   0.5038    
paper                   0.14855    0.15210   0.977   0.3287    
WWWhourspW             -0.02598    0.08008  -0.324   0.7457    
religious               0.05139    0.15274   0.336   0.7365    
employMonths            0.01899    0.07122   0.267   0.7897    
rural                  -0.35097    0.21007  -1.671   0.0948 .  
partly.rural           -0.37978    0.19413  -1.956   0.0504 .  
urban                   0.12732    0.21202   0.601   0.5482    
health.goodfair        -0.09534    0.33856  -0.282   0.7782    
health.goodfairly good  0.11669    0.31240   0.374   0.7087    
health.goodgood         0.02744    0.31895   0.086   0.9314    
HHInc                  -0.48513    0.08447  -5.743 9.30e-09 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1236.9  on 1048  degrees of freedom
Residual deviance: 1143.3  on 1028  degrees of freedom
AIC: 1185.3

Number of Fisher Scoring iterations: 5</code></pre>
<p>There are also two other ways to look at the estimated parameters of our model. We can just call the coefficients or we can exploit that they are an object within the summary object of the model object.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># extract coeffiecients only</span>
<span class="kw">coef</span>(m.log)</code></pre>
<pre><code>           (Intercept)             RSexFemale                   RAge 
            0.72437209             0.64029593             0.01031349 
              Househld                    Lab                    SNP 
            0.02793778            -0.31577137             1.85513031 
                  Ukip                    BNP                     GP 
            0.05604390             0.92131027            -0.51314845 
           party.other                  paper             WWWhourspW 
            0.12542127             0.14855174            -0.02597660 
             religious           employMonths                  rural 
            0.05138795             0.01898927            -0.35096655 
          partly.rural                  urban        health.goodfair 
           -0.37977789             0.12731654            -0.09534035 
health.goodfairly good        health.goodgood                  HHInc 
            0.11669492             0.02743714            -0.48513120 </code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># only estimates table of summary</span>
<span class="kw">summary</span>(m.log)<span class="op">$</span>coef</code></pre>
<pre><code>                          Estimate Std. Error     z value     Pr(&gt;|z|)
(Intercept)             0.72437209 0.36093823  2.00691428 4.475879e-02
RSexFemale              0.64029593 0.15056566  4.25260266 2.113003e-05
RAge                    0.01031349 0.09072502  0.11367858 9.094926e-01
Househld                0.02793778 0.08120874  0.34402436 7.308280e-01
Lab                    -0.31577137 0.19963619 -1.58173411 1.137103e-01
SNP                     1.85513031 1.05602885  1.75670419 7.896824e-02
Ukip                    0.05604390 0.44846178  0.12496918 9.005480e-01
BNP                     0.92131027 0.57304844  1.60773542 1.078931e-01
GP                     -0.51314845 0.46574170 -1.10178763 2.705540e-01
party.other             0.12542127 0.18760248  0.66854805 5.037838e-01
paper                   0.14855174 0.15210121  0.97666373 3.287357e-01
WWWhourspW             -0.02597660 0.08008122 -0.32437813 7.456518e-01
religious               0.05138795 0.15273851  0.33644397 7.365361e-01
employMonths            0.01898927 0.07121623  0.26664249 7.897444e-01
rural                  -0.35096655 0.21006747 -1.67073251 9.477452e-02
partly.rural           -0.37977789 0.19413416 -1.95626517 5.043393e-02
urban                   0.12731654 0.21201600  0.60050437 5.481701e-01
health.goodfair        -0.09534035 0.33855835 -0.28160685 7.782450e-01
health.goodfairly good  0.11669492 0.31239717  0.37354666 7.087416e-01
health.goodgood         0.02743714 0.31895327  0.08602245 9.314486e-01
HHInc                  -0.48513120 0.08447377 -5.74298026 9.302457e-09</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># display p-values only</span>
<span class="kw">summary</span>(m.log)<span class="op">$</span>coef[, <span class="dv">4</span>]</code></pre>
<pre><code>           (Intercept)             RSexFemale                   RAge 
          4.475879e-02           2.113003e-05           9.094926e-01 
              Househld                    Lab                    SNP 
          7.308280e-01           1.137103e-01           7.896824e-02 
                  Ukip                    BNP                     GP 
          9.005480e-01           1.078931e-01           2.705540e-01 
           party.other                  paper             WWWhourspW 
          5.037838e-01           3.287357e-01           7.456518e-01 
             religious           employMonths                  rural 
          7.365361e-01           7.897444e-01           9.477452e-02 
          partly.rural                  urban        health.goodfair 
          5.043393e-02           5.481701e-01           7.782450e-01 
health.goodfairly good        health.goodgood                  HHInc 
          7.087416e-01           9.314486e-01           9.302457e-09 </code></pre>
<p>The parameters may be of interest if inference is our goal. But if we are just interested in classification we would like to make predictions. This can be done directly by using the predict() function:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># predict probabilities</span>
pred.probs &lt;-<span class="st"> </span><span class="kw">predict</span>( m.log, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
pred.probs[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>] <span class="co"># predictions for the first 10 respondents</span></code></pre>
<pre><code>        1         2         3         4         5         6         7 
0.5213789 0.9181186 0.8412570 0.8244665 0.8349719 0.6931744 0.9370885 
        8         9        10 
0.6815346 0.9090575 0.6897961 </code></pre>
<p>To see how good our classification model is we need to compare the classification with the actual outcomes. We first create an object <code>exp.out</code> which will be either <code>0</code> or <code>1</code>. In a second step, we cross-tab it with the true outcomes and this allows us to see how well the classification model is doing.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># predict whether respondent over-estimates or not</span>
exp.out &lt;-<span class="st"> </span><span class="kw">ifelse</span>( pred.probs <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dt">yes =</span> <span class="dv">1</span>, <span class="dt">no =</span> <span class="dv">0</span>)

<span class="co"># confusion matrix (table of predictions and true outcomes)</span>
<span class="kw">table</span>(<span class="dt">prediction =</span> exp.out, <span class="dt">truth =</span> df<span class="op">$</span>over.estimate)</code></pre>
<pre><code>          truth
prediction   0   1
         0  41  40
         1 249 719</code></pre>
<p>The diagonal elements are the correct classifications and the off-diagonal ones are wrong. We can compute the share of correct classified observations as a ratio.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># percent correctly classified</span>
(<span class="dv">41</span> <span class="op">+</span><span class="st"> </span><span class="dv">719</span>) <span class="op">/</span><span class="st"> </span><span class="dv">1049</span></code></pre>
<pre><code>[1] 0.7244995</code></pre>
<p>We can also write code that will estimate the percentage correctly classified for different values.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># more generally</span>
<span class="kw">mean</span>( exp.out <span class="op">==</span><span class="st"> </span>df<span class="op">$</span>over.estimate)</code></pre>
<pre><code>[1] 0.7244995</code></pre>
<p>This is the performance on the training data and we expect the test error to be higher than this. To get at a better indication of the model’s classification error we can split the dataset into a training set and a test set.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set the random number generator</span>
<span class="kw">set.seed</span>(<span class="dv">12</span>)

<span class="co"># random draw of 80% of the observation (row numbers) to train the model</span>
train.ids &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(df), <span class="dt">size =</span> <span class="kw">as.integer</span>( (<span class="kw">nrow</span>(df)<span class="op">*</span>.<span class="dv">80</span>) ), <span class="dt">replace =</span> <span class="ot">FALSE</span>)

<span class="co"># the validation data </span>
df.test &lt;-<span class="st"> </span>df[ <span class="op">-</span>train.ids, ]
<span class="kw">dim</span>(df.test)</code></pre>
<pre><code>[1] 210  23</code></pre>
<p>Now we fit the model using the training data only and then test its performance on the test data.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># re-fit the model on the raining data</span>
m.log &lt;-<span class="st"> </span><span class="kw">glm</span>(over.estimate <span class="op">~</span><span class="st"> </span>RSex <span class="op">+</span><span class="st"> </span>RAge <span class="op">+</span><span class="st"> </span>Househld <span class="op">+</span><span class="st"> </span>Lab <span class="op">+</span><span class="st"> </span>SNP <span class="op">+</span><span class="st"> </span>Ukip <span class="op">+</span><span class="st"> </span>BNP <span class="op">+</span><span class="st"> </span>
<span class="st">               </span>GP <span class="op">+</span><span class="st"> </span>party.other <span class="op">+</span><span class="st"> </span>paper <span class="op">+</span><span class="st"> </span>WWWhourspW <span class="op">+</span><span class="st">  </span>religious <span class="op">+</span><span class="st"> </span>
<span class="st">               </span>employMonths <span class="op">+</span><span class="st"> </span>rural <span class="op">+</span><span class="st"> </span>partly.rural <span class="op">+</span><span class="st"> </span>urban <span class="op">+</span><span class="st"> </span>health.good <span class="op">+</span><span class="st"> </span>
<span class="st">               </span>HHInc, <span class="dt">data =</span> df, <span class="dt">subset =</span> train.ids, 
             <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))

<span class="co"># predict probabilities of over-estimating but for the unseen data</span>
pred.probs &lt;-<span class="st"> </span><span class="kw">predict</span>(m.log, <span class="dt">newdata =</span> df.test, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)

<span class="co"># classify predictions as over-estimating or not</span>
exp.out &lt;-<span class="st"> </span><span class="kw">ifelse</span>( pred.probs <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dt">yes =</span> <span class="dv">1</span>, <span class="dt">no =</span> <span class="dv">0</span>)

<span class="co"># confusion matrix of predictions against truth</span>
<span class="kw">table</span>( <span class="dt">prediction =</span> exp.out, <span class="dt">truth =</span> df.test<span class="op">$</span>over.estimate)</code></pre>
<pre><code>          truth
prediction   0   1
         0   5   7
         1  65 133</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># percent correctly classified</span>
<span class="kw">mean</span>( exp.out <span class="op">==</span><span class="st"> </span>df.test<span class="op">$</span>over.estimate )</code></pre>
<pre><code>[1] 0.6571429</code></pre>
<p>We see that the classification accuracy is too high in the training dataset. The accuracy on the test dataset provides a good estimate of the model’s abbility to correctly identify observations.</p>
<p>Let’s try to improve the classification model by relying on the best predictors.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># try to improve the prediction model by relying on &quot;good&quot; predictors</span>
m.log &lt;-<span class="st"> </span><span class="kw">glm</span>(over.estimate <span class="op">~</span><span class="st"> </span>RSex <span class="op">+</span><span class="st"> </span>rural <span class="op">+</span><span class="st"> </span>partly.rural <span class="op">+</span><span class="st"> </span>urban <span class="op">+</span><span class="st"> </span>HHInc, 
             <span class="dt">data =</span> df, <span class="dt">subset =</span> train.ids, <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))
pred.probs &lt;-<span class="st"> </span><span class="kw">predict</span>(m.log, <span class="dt">newdata =</span> df.test, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
exp.out &lt;-<span class="st"> </span><span class="kw">ifelse</span>( pred.probs <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dt">yes =</span> <span class="dv">1</span>, <span class="dt">no =</span> <span class="dv">0</span>)
<span class="kw">table</span>( <span class="dt">prediction =</span> exp.out, <span class="dt">truth =</span> df.test<span class="op">$</span>over.estimate )</code></pre>
<pre><code>          truth
prediction   0   1
         0   6   4
         1  64 136</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>( exp.out <span class="op">==</span><span class="st"> </span>df.test<span class="op">$</span>over.estimate )</code></pre>
<pre><code>[1] 0.6761905</code></pre>
<p>We see that the classification model’s accurcy increases when we only rely the strongest predictors.</p>
<p>You can also make predictions for specific settings:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># prediction for a specific setting</span>
<span class="kw">predict</span>(m.log, <span class="dt">newdata =</span> <span class="kw">data.frame</span>( <span class="dt">RSex =</span> <span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>),
                                     <span class="dt">rural =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>),
                                     <span class="dt">partly.rural =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>),
                                     <span class="dt">urban =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>),
                                     <span class="dt">HHInc =</span> <span class="kw">c</span>(<span class="dv">9</span>, <span class="dv">9</span>)), <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</code></pre>
<pre><code>         1          2 
0.02372597 0.04504885 </code></pre>
<div id="model-the-underlying-continuous-process" class="section level3">
<h3><span class="header-section-number">10.1.1</span> Model the Underlying Continuous Process</h3>
<p>Lastly, we can try to model the underlying process and classify afterwards. By doing that, the depdendent variable provides more information. In effect we turn our classification problem into a regression problem.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit the linear model on the numer of immigrants per 100 Brits</span>
m.lm &lt;-<span class="st"> </span><span class="kw">lm</span>(IMMBRIT <span class="op">~</span><span class="st"> </span>RSex <span class="op">+</span><span class="st"> </span>rural <span class="op">+</span><span class="st"> </span>partly.rural <span class="op">+</span><span class="st"> </span>urban <span class="op">+</span><span class="st"> </span>HHInc, 
            <span class="dt">data =</span> df, <span class="dt">subset =</span> train.ids)

<span class="co"># predict</span>
y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(m.lm, <span class="dt">newdata =</span> df.test)

<span class="co"># threshold for classfication</span>
threshold &lt;-<span class="st"> </span>(<span class="fl">10.7</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(df<span class="op">$</span>IMMBRIT_original_scale)) <span class="op">/</span><span class="st"> </span><span class="kw">sd</span>(df<span class="op">$</span>IMMBRIT_original_scale)

<span class="co"># now we do the classfication </span>
exp.out &lt;-<span class="st"> </span><span class="kw">ifelse</span>( y_hat <span class="op">&gt;</span><span class="st"> </span>threshold, <span class="dt">yes =</span> <span class="dv">1</span>, <span class="dt">no =</span> <span class="dv">0</span>)

<span class="co"># confusion matrix</span>
<span class="kw">table</span>( <span class="dt">prediction =</span> exp.out, <span class="dt">truth =</span> df.test<span class="op">$</span>over.estimate)</code></pre>
<pre><code>          truth
prediction   0   1
         1  70 140</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># percent correctly classified</span>
<span class="kw">mean</span>( exp.out <span class="op">==</span><span class="st"> </span>df.test<span class="op">$</span>over.estimate)</code></pre>
<pre><code>[1] 0.6666667</code></pre>
<p>We do worse by treating this as a regression problem rather than a classification problem.</p>
</div>
<div id="leave-one-out-cross-validation" class="section level3">
<h3><span class="header-section-number">10.1.2</span> Leave-One-Out-Cross-Validation</h3>
<p>The <code>glm()</code> function offers a generalization of the linear model while allowing for different link functions and error distributions other than gaussian. By default, <code>glm()</code> simply fits a linear model identical to the one estimated with <code>lm()</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># linear regression fitted with glm() and lm()</span>
glm.fit &lt;-<span class="st"> </span><span class="kw">glm</span>( IMMBRIT <span class="op">~</span><span class="st"> </span>RAge, <span class="dt">data =</span> df)
lm.fit &lt;-<span class="st"> </span><span class="kw">lm</span>( IMMBRIT <span class="op">~</span><span class="st"> </span>RAge, <span class="dt">data =</span> df)</code></pre>
<p>The <code>glm()</code> function can be used with <code>cv.glm()</code> to estimate k-fold cross-validation prediction error.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># use cv.glm() for k-fold corss-validation on glm</span>
<span class="kw">library</span>(boot)
cv.err &lt;-<span class="st"> </span><span class="kw">cv.glm</span>(df, glm.fit)
<span class="co"># cross-validation error</span>
cv.err<span class="op">$</span>delta</code></pre>
<pre><code>[1] 1.001291 1.001289</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># the number of folds</span>
cv.err<span class="op">$</span>K</code></pre>
<pre><code>[1] 1049</code></pre>
<p>The returned value from <code>cv.glm()</code> contains a delta vector of components - the raw cross-validation estimate and the adjusted cross-validation estimate respectively. We are interested in the raw cross-validation error.</p>
<p>NOTE: if we do not provide the option <strong>K</strong> in <code>cv.glm()</code> we automatically perfrom LOOCV.</p>
<p>We can repeat this process in a <code>for()</code> loop to compare the cross-validation error of higher-order polynomials. The following example estimates the polynomial fit of the order 1 through 7 and stores the result in a cv.error vector.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># container for cv errors</span>
cv.error &lt;-<span class="st"> </span><span class="ot">NA</span>

<span class="co"># loop over age raised to the power 1...7</span>
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>){
  glm.fit &lt;-<span class="st"> </span><span class="kw">glm</span>( IMMBRIT <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(RAge, i), <span class="dt">data =</span> df )
  cv.error[i] &lt;-<span class="st"> </span><span class="kw">cv.glm</span>(df, glm.fit)<span class="op">$</span>delta[<span class="dv">1</span>]
}
cv.error</code></pre>
<pre><code>[1] 1.0012907 0.9932016 0.9840021 0.9854419 0.9869106 0.9876803 0.9888996</code></pre>
<p>We plot the effect of increasing the complexity of the model</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot of error rates</span>
<span class="kw">plot</span>(<span class="dt">x =</span> <span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">7</span>),
     <span class="dt">y =</span> cv.error,
     <span class="dt">bty =</span> <span class="st">&quot;n&quot;</span>, 
     <span class="dt">pch =</span> <span class="dv">20</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;complexity&quot;</span>, 
     <span class="dt">ylab =</span> <span class="st">&quot;cross-validation error&quot;</span>,
     <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="fl">0.98</span>, <span class="fl">1.01</span>))
<span class="kw">lines</span>( <span class="dt">y =</span> cv.error, <span class="dt">x =</span> <span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">7</span>), <span class="dt">lwd =</span> <span class="dv">2</span>)</code></pre>
<p><img src="suffolk2019_files/figure-html/non.finished.plotting-1.png" width="672" /></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="seminar9.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": {}
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["suffolk2019.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
