# Prediction and assessing prediction accuracy

```{r, include = FALSE}
par(bg = '#fdf6e3')
```

## Seminar

In this seminar we will cover classification, and assess model accuracy out-of-sample.

The non-western foreingers data is about the subjective perception of immigrants from non-western countries. The perception of immigrants from a context that is not similar to the one's own ,is often used as a proxy for racism. Whether this is a fair measure or not is debatable but let's examine the data from a survey carried out in Britain. 

Let's check the codebook of our data.

```{r echo = FALSE}
knitr::kable(tibble::tribble(
  ~Variable,     ~Description,
  "IMMBRIT",    "Out of every 100 people in Britain, how many do you think are immigrants from non-western countries?",
  "over.estimate",      "1 if estimate is higher than 10.7%.",
  "RSex",      "1 = male, 2 = female",
  "RAge",      "Age of respondent",
  "Househld",    "Number of people living in respondent's household",
  "party identification",    "1 = Conservatives, 2 = Labour, 3 = SNP, 4 = Greens, 5 = Ukip, 6 = BNP, 7 = other",
  "paper",    "Do you normally read any daily morning newspaper 3+ times/week?",
  "WWWhourspW",       "How many hours WWW per week?",
  "religious",    "Do you regard yourself as belonging to any particular religion?",
  "employMonths", "How many mnths w. present employer?",
  "urban",      "Population density, 4 categories (highest density is 4, lowest is 1)",
  "health.good",     "How is your health in general for someone of your age? (0: bad, 1: fair, 2: fairly good, 3: good)",
  "HHInc",        "Income bands for household, high number = high HH income"
))
```

Let's load the dataset.

```{r}
df <- read.csv("non_western_immigrants.csv", stringsAsFactors = FALSE)
```

Declaring categorical variables.

```{r}
# data manipulation
df$RSex <- factor(df$RSex, labels = c("Male", "Female"))
df$health.good <- factor(df$health.good, labels = c("bad", "fair", "fairly good", "good") )
```

We want to predict whether respondents over-estimate immigration from non-western contexts. We begin by normalizing our variables. Then we look at the distribution of the dependent variable. We check how well we could predict misperception of immigration in our sample without a statistical model.


```{r}
# create a copy of the original IMMBRIT variable (needed for classification with lm)
df$IMMBRIT_original_scale <- df$IMMBRIT

# our function for normalization
our.norm <- function(x){
  return((x - mean(x)) / sd(x))
}

# continuous variables
c.vars <- c("IMMBRIT", "RAge", "Househld", "HHInc", "employMonths", "WWWhourspW")

# normalize
df[, c.vars] <- apply( df[, c.vars], 2, our.norm )

# predict whether poeple overestimate rate of immigrants (i.e. more than 10.7%)
table(df$over.estimate)

# probability of misperception of immigration in the sample
mean(df$over.estimate)  

# naive guess
median(df$over.estimate)
```

Create dummy variables from our categorical variables.

```{r}
df$Cons <- ifelse(df$party_self == 1, yes = 1, no = 0)
df$Lab <- ifelse(df$party_self == 2, yes = 1, no = 0)
df$SNP <- ifelse(df$party_self == 3, yes = 1, no = 0)
df$GP <- ifelse(df$party_self == 4, yes = 1, no = 0)
df$BNP <- ifelse(df$party_self == 6, yes = 1, no = 0)
df$Ukip <- ifelse(df$party_self == 5, yes = 1, no = 0)
df$party.other <- ifelse(df$party_self == 7, yes = 1, no = 0)
df$rural <- ifelse(df$urban == 1, yes = 1, no = 0)
df$partly.rural <- ifelse(df$urban == 2, yes = 1, no = 0)
df$urban <- ifelse(df$urban == 4, yes = 1, no = 0)
```


Now, we fit a logistic regression.

```{r}
# run logistic regression
m.log <- glm(over.estimate ~ RSex + RAge + Househld + Lab + SNP + Ukip + BNP + 
               GP + party.other + paper + WWWhourspW +  religious + 
               employMonths + rural + partly.rural + urban + 
               health.good + HHInc, data = df,
             family = binomial(link = "logit"))
summary(m.log)
```


There are also two other ways to look at the estimated parameters of our model. We can just call the coefficients or we can exploit that they are an object within the summary object of the model object.

```{r}
# extract coeffiecients only
coef(m.log)

# only estimates table of summary
summary(m.log)$coef

# display p-values only
summary(m.log)$coef[, 4]
```

The parameters may be of interest if inference is our goal. But if we are just interested in classification we would like to make predictions. This can be done directly by using the predict() function:

```{r}
# predict probabilities
pred.probs <- predict( m.log, type = "response")
pred.probs[1:10] # predictions for the first 10 respondents
```

To see how good our classification model is we need to compare the classification with the actual outcomes. We first create an object `exp.out` which will be either `0` or `1`. In a second step, we cross-tab it with the true outcomes and this allows us to see how well the classification model is doing.

```{r}
# predict whether respondent over-estimates or not
exp.out <- ifelse( pred.probs > 0.5, yes = 1, no = 0)

# confusion matrix (table of predictions and true outcomes)
table(prediction = exp.out, truth = df$over.estimate)
```

The diagonal elements are the correct classifications and the off-diagonal ones are wrong. We can compute the share of correct classified observations as a ratio.

```{r}
# percent correctly classified
(41 + 719) / 1049
```

We can also write code that will estimate the percentage correctly classified for different values.

```{r}
# more generally
mean( exp.out == df$over.estimate)
```

This is the performance on the training data and we expect the test error to be higher than this. To get at a better indication of the model's classification error we can split the dataset into a training set and a test set.

```{r}
# set the random number generator
set.seed(12)

# random draw of 80% of the observation (row numbers) to train the model
train.ids <- sample(nrow(df), size = as.integer( (nrow(df)*.80) ), replace = FALSE)

# the validation data 
df.test <- df[ -train.ids, ]
dim(df.test)
```

Now we fit the model using the training data only and then test its performance on the test data.

```{r}
# re-fit the model on the raining data
m.log <- glm(over.estimate ~ RSex + RAge + Househld + Lab + SNP + Ukip + BNP + 
               GP + party.other + paper + WWWhourspW +  religious + 
               employMonths + rural + partly.rural + urban + health.good + 
               HHInc, data = df, subset = train.ids, 
             family = binomial(link = "logit"))

# predict probabilities of over-estimating but for the unseen data
pred.probs <- predict(m.log, newdata = df.test, type = "response")

# classify predictions as over-estimating or not
exp.out <- ifelse( pred.probs > 0.5, yes = 1, no = 0)

# confusion matrix of predictions against truth
table( prediction = exp.out, truth = df.test$over.estimate)

# percent correctly classified
mean( exp.out == df.test$over.estimate )
```

We see that the classification accuracy is too high in the training dataset. The accuracy on the test dataset provides a good estimate of the model's abbility to correctly identify observations.

Let's try to improve the classification model by relying on the best predictors.

```{r}
# try to improve the prediction model by relying on "good" predictors
m.log <- glm(over.estimate ~ RSex + rural + partly.rural + urban + HHInc, 
             data = df, subset = train.ids, family = binomial(link = "logit"))
pred.probs <- predict(m.log, newdata = df.test, type = "response")
exp.out <- ifelse( pred.probs > 0.5, yes = 1, no = 0)
table( prediction = exp.out, truth = df.test$over.estimate )
mean( exp.out == df.test$over.estimate )
```

We see that the classification model's accurcy increases when we only rely the strongest predictors.

You can also make predictions for specific settings:

```{r}
# prediction for a specific setting
predict(m.log, newdata = data.frame( RSex = c("Male", "Female"),
                                     rural = c(0, 0),
                                     partly.rural = c(0, 0),
                                     urban = c(0, 0),
                                     HHInc = c(9, 9)), type = "response")
```

### Model the Underlying Continuous Process

Lastly, we can try to model the underlying process and classify afterwards. By doing that, the depdendent variable provides more information. In effect we turn our classification problem into a regression problem.

```{r}
# fit the linear model on the numer of immigrants per 100 Brits
m.lm <- lm(IMMBRIT ~ RSex + rural + partly.rural + urban + HHInc, 
            data = df, subset = train.ids)

# predict
y_hat <- predict(m.lm, newdata = df.test)

# threshold for classfication
threshold <- (10.7 - mean(df$IMMBRIT_original_scale)) / sd(df$IMMBRIT_original_scale)

# now we do the classfication 
exp.out <- ifelse( y_hat > threshold, yes = 1, no = 0)

# confusion matrix
table( prediction = exp.out, truth = df.test$over.estimate)

# percent correctly classified
mean( exp.out == df.test$over.estimate)
```

We do worse by treating this as a regression problem rather than a classification problem.

### Leave-One-Out-Cross-Validation

The `glm()` function offers a generalization of the linear model while allowing for different link functions and error distributions other than gaussian. By default, `glm()` simply fits a linear model identical to the one estimated with `lm()`.

```{r}
# linear regression fitted with glm() and lm()
glm.fit <- glm( IMMBRIT ~ RAge, data = df)
lm.fit <- lm( IMMBRIT ~ RAge, data = df)
```

The `glm()` function can be used with `cv.glm()` to estimate k-fold cross-validation prediction error.

```{r}
# use cv.glm() for k-fold corss-validation on glm
library(boot)
cv.err <- cv.glm(df, glm.fit)
# cross-validation error
cv.err$delta
# the number of folds
cv.err$K
```

The returned value from `cv.glm()` contains a delta vector of components - the raw cross-validation estimate and the adjusted cross-validation estimate respectively. We are interested in the raw cross-validation error.

NOTE: if we do not provide the option **K** in `cv.glm()` we automatically perfrom LOOCV.

We can repeat this process in a `for()` loop to compare the cross-validation error of higher-order polynomials. The following example estimates the polynomial fit of the order 1 through 7 and stores the result in a cv.error vector.

```{r}
# container for cv errors
cv.error <- NA

# loop over age raised to the power 1...7
for (i in 1:7){
  glm.fit <- glm( IMMBRIT ~ poly(RAge, i), data = df )
  cv.error[i] <- cv.glm(df, glm.fit)$delta[1]
}
cv.error
```

We plot the effect of increasing the complexity of the model

```{r, non.finished.plotting}
# plot of error rates
plot(x = seq(1, 7),
     y = cv.error,
     bty = "n", 
     pch = 20,
     xlab = "complexity", 
     ylab = "cross-validation error",
     ylim = c(0.98, 1.01))
lines( y = cv.error, x = seq(1,7), lwd = 2)
```
